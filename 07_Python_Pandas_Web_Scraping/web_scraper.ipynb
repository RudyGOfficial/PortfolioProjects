{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "521b5713",
   "metadata": {},
   "source": [
    "# Scraping Data From A Web Page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db10afb",
   "metadata": {},
   "source": [
    "This notebook scrapes tables from a specific Wikipedia page and stores them in this notebook's directory as an Excel workbook or CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2acb5f",
   "metadata": {},
   "source": [
    "#### 1. Import Python modules to access the necessary functions for converting web tables to Excel/CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce52f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # HTTP requests to a chosen URL\n",
    "from bs4 import BeautifulSoup  # HTML parsing & storage\n",
    "import pandas as pd  # DataFrame creation of scraped tables\n",
    "import os  # Operating system functionalities such as getting file paths\n",
    "import xlsxwriter  # Excel file writing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6451a511",
   "metadata": {},
   "source": [
    "#### 2. Hard-code variables for use by the WebScraper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726c0e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_largest_companies_in_the_United_States_by_revenue'\n",
    "markup = 'html'  # Markup language of web page\n",
    "file_path = os.getcwd() + \"\\\\\"  # This notebook's file path\n",
    "file_extensions = ['.csv', '.xlsx']  # Acceptable extensions for exporting\n",
    "excel = True  # Exports Excel file (True) or CSVs (False)\n",
    "excel_name = \"Largest_US_Companies_by_Revenue\"  # Excle file custom name\n",
    "sheet_names = ['Public_Companies',\n",
    "               'Private_Companies',\n",
    "               'Companies_by_Profit']  # Custom names for Excel sheets or CSV file names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40597e82",
   "metadata": {},
   "source": [
    "#### 3. Encapsulate all web scraping functions into a WebScraper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e5c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraper():\n",
    "    # Assign values to instance variables when WebScraper object is initialized\n",
    "    def __init__(self, url, markup, file_path, file_extensions,\n",
    "                 excel=False, excel_name=\"workbook\", sheet_names=['sheet']):\n",
    "        self.url = url\n",
    "        self.markup = markup\n",
    "        self.file_path = file_path\n",
    "        self.file_extensions = file_extensions\n",
    "        self.excel = excel\n",
    "        self.excel_name = excel_name\n",
    "        self.sheet_names = sheet_names\n",
    "        self.page = None\n",
    "        self.soup = None\n",
    "        self.df_list = []\n",
    "    \n",
    "    # Conduct HTTP Request and store resulting markup as text in the soup instance variable\n",
    "    def MakeSoup(self):\n",
    "        self.page = requests.get(self.url)\n",
    "        self.soup = BeautifulSoup(self.page.text, self.markup)\n",
    "        return self.soup\n",
    "    \n",
    "    # Scrape the soup variable for tables and store them in the dataframe list instance variable\n",
    "    def BuildDataFrames(self):\n",
    "        \n",
    "        # Find all tables by the 'table' tag\n",
    "        table_list = self.soup.find_all('table')\n",
    "        \n",
    "        # Iterate through the table list\n",
    "        for table in table_list:\n",
    "            \n",
    "            # Find all headers of the current table by the 'th' tag\n",
    "            raw_headers = table.find_all('th')\n",
    "            \n",
    "            # Skip Tables with zero headers & begin next iteration\n",
    "            if len(raw_headers) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Remove markup tags & strip whitespace from each header\n",
    "            clean_headers = [header.text.strip() for header in raw_headers]\n",
    "            \n",
    "            # Initiate a DataFrame with the clean headers\n",
    "            df = pd.DataFrame(columns = clean_headers)\n",
    "\n",
    "            # Find all rows of the current table by the 'tr' tag\n",
    "            table_rows = table.find_all('tr')\n",
    "            \n",
    "            # Iterate through the table's rows\n",
    "            for row in table_rows:\n",
    "\n",
    "                # Find all values in the current row of the current table\n",
    "                raw_row_data = row.find_all('td')\n",
    "                \n",
    "                # Skip invalid rows & begin next iteration\n",
    "                if len(raw_row_data) == 0 or len(raw_row_data) != len(raw_headers):\n",
    "                    continue\n",
    "                \n",
    "                # Remove markup tags & strip whitespace from each value\n",
    "                clean_row_data = [value.text.strip() for value in raw_row_data]\n",
    "\n",
    "                # Assign the current clean row's values to the end of the DataFrame\n",
    "                # The last index is found by calculating the DataFrame's length\n",
    "                length = len(df)  # Get length\n",
    "                df.loc[length] = clean_row_data  # Assign clean row to end\n",
    "            \n",
    "            # Only adds non-empty DataFrames to the DataFrame list\n",
    "            if len(df) > 0:\n",
    "                self.df_list.append(df)\n",
    "        \n",
    "        return self.df_list\n",
    "    \n",
    "    # Build a list of sheet names for each DataFrame in the DataFrame list instance variable\n",
    "    def BuildSheetNames(self):\n",
    "        self.sheet_names = []\n",
    "        i = 1\n",
    "        while len(self.df_list) != len(self.sheet_names):\n",
    "            self.sheet_names.append(f\"sheet_{i}\")\n",
    "            i += 1\n",
    "    \n",
    "    # Export the DataFrames as an Excel workbook or CSV files\n",
    "    def ExportDataFrames(self):\n",
    "        \n",
    "        # Build sheet names if its count does not match the DataFrame list's count\n",
    "        if len(self.df_list) != len(self.sheet_names):\n",
    "            self.BuildSheetNames()\n",
    "        \n",
    "        # Initialize an ExcelWriter object if user preference is to export as Excel\n",
    "        if self.excel:\n",
    "            # Writes & opens a new Excel workbook in the chosen File Path\n",
    "            writer = pd.ExcelWriter(self.file_path + self.excel_name + self.file_extensions[1], engine='xlsxwriter')\n",
    "        \n",
    "        # Iterate through each DataFrame in the DataFrame list instance variable\n",
    "        i = 0\n",
    "        for df in self.df_list:\n",
    "            # Writes current DataFrame to an Excel sheet or CSV file\n",
    "            if self.excel:\n",
    "                df.to_excel(writer, sheet_name=self.sheet_names[i], index=False)\n",
    "            else:\n",
    "                df.to_csv(self.file_path + self.sheet_names[i] + self.file_extensions[0], index = False)\n",
    "            i += 1\n",
    "        \n",
    "        # Closes Excel workbook if Excel was chosen & opened by the ExcelWriter object\n",
    "        if self.excel:\n",
    "            writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d781e75",
   "metadata": {},
   "source": [
    "#### 4. Initialize WebScraper object and run its functions to export web tables into Excel/CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a0a02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize WebScraper object with the hard-coded variables\n",
    "webscraper = WebScraper(url, markup, file_path, file_extensions, \n",
    "                        excel, excel_name, sheet_names)\n",
    "\n",
    "# Run the WebScraper functions\n",
    "webscraper.MakeSoup()  # Gets markup & stores as soup\n",
    "webscraper.BuildDataFrames()  # Converts markup tables into DataFrame objects\n",
    "webscraper.ExportDataFrames()  # Exports DataFrames based on user preferred file extension"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
